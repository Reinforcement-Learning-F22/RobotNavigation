{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyiN4d52a0pC"
      },
      "source": [
        "# Deep Q-Network (DQN)\n",
        "\n",
        "**Description:** Implementing DQN algorithm on the Robot Navigation Problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ryZIi8a0pI"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "**Deep Q-Network (DQN)** is a model-free off-policy algorithm for learning discrete actions.\n",
        "\n",
        "It uses neural networks to apply function approximation, to estimate the Q-function while avoiding convergence problems in the case of continous actions space, states space or both.\n",
        "\n",
        "It uses a method called replayed memory, which represents a memory buffer for storing the experiences of the agent.\n",
        "\n",
        "## Problem\n",
        "\n",
        "We are trying to solve the **Robot Navigation** problem.\n",
        "In this setting, we can assume taking three actions: [Forward, LeftTurn, RightTurn].\n",
        "\n",
        "\n",
        "The implementation of the algorithm is carried out in the rest of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HMEkgsIva0pL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-04 20:11:06.065597: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-04 20:11:06.149397: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2022-11-04 20:11:06.152773: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:\n",
            "2022-11-04 20:11:06.152784: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2022-11-04 20:11:06.170248: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-11-04 20:11:06.503186: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:\n",
            "2022-11-04 20:11:06.503238: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:\n",
            "2022-11-04 20:11:06.503243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import gym\n",
        "import bot3RLNav\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.n_actions = action_size\n",
        "        '''\n",
        "        # \"lr\" : learning rate\n",
        "        # \"gamma\": discount factor\n",
        "        # \"decay_rate\": exponential decay rate of the exploration probability\n",
        "        # \"batch_size\": size of the sampled experiences to train the DNN\n",
        "        # \"epsilon:\" exploration probability\n",
        "        '''\n",
        "        self.lr = 0.001\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.decay_rate = 0.005\n",
        "        self.batch_size = 32\n",
        "        \n",
        "        # Define memory buffer for experiences storage\n",
        "        self.memory_buffer= list()\n",
        "        # Store the last 2000  time steps\n",
        "        self.max_memory_buffer = 2000\n",
        "        \n",
        "        # Create a model having two hidden layers of 24 units\n",
        "        # The first layer has the size of \"state space\"\n",
        "        # The last layer has the size of \"actions space\"\n",
        "        self.model = Sequential([\n",
        "            Dense(units=24,input_dim=state_size, activation = 'relu'),\n",
        "            Dense(units=24,activation = 'relu'),\n",
        "            Dense(units=action_size, activation = 'linear')\n",
        "        ])\n",
        "        self.model.compile(loss=\"mse\", optimizer = Adam(lr=self.lr))\n",
        "\n",
        "    def get_action(self, current_state):\n",
        "        if np.random.uniform(0,1) < self.epsilon:\n",
        "            return np.random.choice(range(self.n_actions))\n",
        "        q_values = self.model.predict(current_state)[0]\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        # Update epsilon at episode termination\n",
        "        self.epsilon = self.epsilon*np.exp(-self.decay_rate)\n",
        "        print(\"epsilon:\", self.epsilon)\n",
        "\n",
        "    def store_episode(self, current_state, action, reward, next_state, done):\n",
        "        self.memory_buffer.append({\n",
        "            \"current_state\":current_state,\n",
        "            \"action\":action,\n",
        "            \"reward\":reward,\n",
        "            \"next_state\":next_state,\n",
        "            \"done\":done,\n",
        "        })\n",
        "        # @ memory buffer size exceeds the max. size, remove the first element (oldest)\n",
        "        if len(self.memory_buffer) > self.max_memory_buffer:\n",
        "            self.memory_buffer.pop(0)\n",
        "\n",
        "    def train(self, batch_size):\n",
        "        # Select batch size of experience, after shuffling the memory buffer\n",
        "        np.random.shuffle(self.memory_buffer)\n",
        "        batch_sample = self.memory_buffer[0:self.batch_size] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sohaila/.local/lib/python3.10/site-packages/gym/envs/registration.py:619: UserWarning: \u001b[33mWARN: Env check failed with the following message: The observation returned by the `reset()` method does not match the given observation space\n",
            "You can set `disable_env_checker=True` to disable this check.\u001b[0m\n",
            "  logger.warn(\n",
            "2022-11-04 20:11:08.273055: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu/gazebo-11/plugins:/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib:/usr/lib/x86_64-linux-gnu/gazebo-11/plugins:\n",
            "2022-11-04 20:11:08.273075: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-11-04 20:11:08.273086: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sohaila-Legion): /proc/driver/nvidia/version does not exist\n",
            "2022-11-04 20:11:08.273246: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/lib/python3.10/random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
            "  return self.randrange(a, b+1)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('bot3RLNav/DiscreteWorld-v1', map_file=\"data/map.jpg\", robot_file=\"data/robot.png\")\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "n_episodes = 150\n",
        "max_iter = 500\n",
        "\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "total_steps = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "episode_rewards = []\n",
        "mean_rewards = []\n",
        "for episode in range(n_episodes):\n",
        "    current_state = env.reset()\n",
        "    current_state = np.array([current_state])\n",
        "\n",
        "    total_rewards_per_episode = 0\n",
        "    for step in range(max_iter):\n",
        "        total_steps += 1\n",
        "        name = \"bot3\"\n",
        "        cv2.namedWindow(name)\n",
        "        rate = 500  # frame rate in ms\n",
        "        frame = env.render(mode=\"rgb_array\")\n",
        "        cv2.imshow(\"bot3\", frame)\n",
        "        cv2.waitKey(rate)\n",
        "\n",
        "        action = agent.get_action(current_state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.array([next_state])\n",
        "        agent.store_episode(current_state, action, reward, next_state, done)\n",
        "        total_rewards_per_episode += reward\n",
        "\n",
        "        if done:\n",
        "            agent.update_epsilon()\n",
        "            break\n",
        "        current_state = next_state\n",
        "  \n",
        "    episode_rewards.append(total_rewards_per_episode)\n",
        "    total_mean_rewards = np.mean(episode_rewards)\n",
        "    mean_rewards.append(total_mean_rewards)\n",
        "    print(\"Episode: \", episode)\n",
        "    print(\"Rewards mean: \", total_mean_rewards)\n",
        "\n",
        "    if total_steps >= agent.batch_size:\n",
        "        agent.train(batch_size=agent.batch_size)\n",
        "\n",
        "# Plot rewards mean against episodes\n",
        "plt.plot(mean_rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Mean rewards\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
