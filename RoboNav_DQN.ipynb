{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyiN4d52a0pC"
      },
      "source": [
        "# Deep Q-Network (DQN)\n",
        "\n",
        "**Description:** Implementing DQN algorithm on the Robot Navigation Problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ryZIi8a0pI"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "**Deep Q-Network (DQN)** is a model-free off-policy algorithm for learning discrete actions.\n",
        "\n",
        "It uses neural networks to apply function approximation, to estimate the Q-function while avoiding convergence problems in the case of continous actions space, states space or both.\n",
        "\n",
        "It uses a method called replayed memory, which represents a memory buffer for storing the experiences of the agent.\n",
        "\n",
        "## Problem\n",
        "\n",
        "We are trying to solve the **Robot Navigation** problem.\n",
        "In this setting, we can assume taking three actions: [Forward, LeftTurn, RightTurn].\n",
        "\n",
        "\n",
        "The implementation of the algorithm is carried out in the rest of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMEkgsIva0pL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.n_actions = action_size\n",
        "        '''\n",
        "        # \"lr\" : learning rate\n",
        "        # \"gamma\": discount factor\n",
        "        # \"decay_rate\": exponential decay rate of the exploration probability\n",
        "        # \"batch_size\": size of the sampled experiences to train the DNN\n",
        "        # \"epsilon:\" exploration probability\n",
        "        '''\n",
        "        self.lr = 0.001\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.decay_rate = 0.005\n",
        "        self.batch_size = 32\n",
        "        \n",
        "        # Define memory buffer for experiences storage\n",
        "        self.memory_buffer= list()\n",
        "        # Store the last 2000  time steps\n",
        "        self.max_memory_buffer = 2000\n",
        "        \n",
        "        # Create a model having two hidden layers of 24 units\n",
        "        # The first layer has the size of \"state space\"\n",
        "        # The last layer has the size of \"actions space\"\n",
        "        self.model = Sequential([\n",
        "            Dense(units=24,input_dim=state_size, activation = 'relu'),\n",
        "            Dense(units=24,activation = 'relu'),\n",
        "            Dense(units=action_size, activation = 'linear')\n",
        "        ])\n",
        "        self.model.compile(loss=\"mse\", optimizer = Adam(lr=self.lr))\n",
        "\n",
        "        def get_action(self, current_state):\n",
        "            if np.random.uniform(0,1) < self.epsilon:\n",
        "                return np.random.choice(range(self.n_actions))\n",
        "            q_values = self.model.predict(current_state)[0]\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "        def update_epsilon(self):\n",
        "            self.epsilon = self.epsilon*np.exp(-self.decay_rate)\n",
        "            print(self.epsilon)\n",
        "\n",
        "        def store_episode(self, current_state, action, reward, next_state, done):\n",
        "            self.memory_buffer.append({\n",
        "                \"current_state\":current_state,\n",
        "                \"action\":action,\n",
        "                \"reward\":reward,\n",
        "                \"next_state\":next_state,\n",
        "                \"done\":done,\n",
        "            })\n",
        "            # @ memory buffer size exceeds the max. size, remove the first element (oldest)\n",
        "            if len(self.memory_buffer) > self.max_memory_buffer:\n",
        "                self.memory_buffer.pop(0)\n",
        "\n",
        "        def train(self):\n",
        "            # Select batch size of experience, after shuffling the memory buffer\n",
        "            np.random.shuffle(self.memory_buffer)\n",
        "            batch_sample = self.memory_buffer[0:self.batch_size] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('bot3RLNav/DiscreteWorld-v0', map_file=\"data/gray.jpg\")\n",
        "prev_state = env.reset()\n",
        "x = [x.tolist() for x in prev_state.values()]\n",
        "print(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
